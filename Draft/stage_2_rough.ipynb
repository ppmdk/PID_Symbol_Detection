{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba948dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, cv2, os\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import albumentations as A\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e851d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m_symbols(classes_dir, m=1, to_run = False):\n",
    "    \n",
    "    print(f\"Getting {m} symbols randomly per class\")\n",
    "    subfolder_pths, subfolder_names, fnames = zip(*[(dirpath, dirnames, filenames) for dirpath, dirnames, filenames in os.walk(classes_dir)])\n",
    "    subfolder_pths = subfolder_pths[1:]; subfolder_names = subfolder_names[0]; fnames = fnames[1:]\n",
    "    symbols = []\n",
    "    targets = []\n",
    "    for sub_pth, sub_name, sub_files in zip(subfolder_pths,subfolder_names,fnames):\n",
    "        rand_m_symbols = random.sample(sub_files, m)\n",
    "        rand_m_symbols_fpth = [os.path.join(sub_pth, o) for o in rand_m_symbols]\n",
    "        symbols.append(rand_m_symbols_fpth)\n",
    "        targets.append([int(sub_name)]*len(rand_m_symbols))\n",
    "\n",
    "    symbols = [element for ls in symbols for element in ls]# flattent the list\n",
    "    targets = [element for ls in targets for element in ls]# flattent the list\n",
    "    print(f\"Selected {len(symbols)} symbols and len of targets is also {len(targets)}\")\n",
    "\n",
    "    targ_names = list(Counter(targets).keys())\n",
    "    targ_counts = list(Counter(targets).values())    \n",
    "    print('******** DONE *************')\n",
    "    return symbols, targets\n",
    "\n",
    "\n",
    "def get_support_set_dict(support_symbols, support_targets):\n",
    "    support_set_dict = {}\n",
    "    for pth,label in zip(support_symbols, support_targets):\n",
    "        if label not in support_set_dict:\n",
    "            support_set_dict[label] = []\n",
    "        support_set_dict[label].append(pth)\n",
    "    return support_set_dict\n",
    "\n",
    "def augment_data(support_set_dict, num_per_class, augmentation_scheme):\n",
    "    \"\"\" works for 1 shot or 2 shot or 5 shot support set\"\"\"\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for label in list(support_set_dict.keys()):\n",
    "        for i in range(num_per_class):\n",
    "            rand_pth = random.choice(support_set_dict[label])\n",
    "            im = cv2.imread(rand_pth)\n",
    "            transformed_im = augmentation_scheme(image = im)\n",
    "            augmented_images.append(transformed_im['image'])\n",
    "            augmented_labels.append(label)\n",
    "    return augmented_images, augmented_labels\n",
    "\n",
    "class One_shot_Dataset_st_list(Dataset):\n",
    "    ''' PID dataset from symbols array list & targets list'''\n",
    "    def __init__(self, symbols, targets, transform=None):\n",
    "        self.symbols = symbols\n",
    "        self.targets = targets \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.symbols)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        symbol_image = Image.fromarray(self.symbols[idx])\n",
    "        if self.transform:\n",
    "            symbol_image = self.transform(symbol_image)\n",
    "        label = self.targets[idx]\n",
    "        return symbol_image, label \n",
    "    \n",
    "class One_shot_Dataset_test(Dataset):\n",
    "    ''' PID dataset from symbols paths list & targets list'''\n",
    "    def __init__(self, symbols, targets, transform=None):\n",
    "        self.symbols = symbols\n",
    "        self.targets = targets \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.symbols)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        symbol_image = cv2.imread(str(self.symbols[idx]))\n",
    "        symbol_image = Image.fromarray(symbol_image)\n",
    "        if self.transform:\n",
    "            symbol_image = self.transform(symbol_image)\n",
    "\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        return symbol_image, label\n",
    "    \n",
    "\n",
    "\n",
    "#  accuracy\n",
    "def calculate_accuracy(output, target):\n",
    "    _, predictions = torch.max(output, 1)\n",
    "    corrects = (predictions == target).float().sum()\n",
    "    return corrects / output.size(0)\n",
    "\n",
    "\n",
    "### Inferencing #####\n",
    "\n",
    "def extract_embeddings(model, data_loader):\n",
    "    model = model.to(device)\n",
    "    model = nn.Sequential(*list(model.children())[:-1]) \n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_ims, batch_labels, _, _ in data_loader:\n",
    "            outputs = model(batch_ims.to(device))\n",
    "            embeddings.append(outputs)\n",
    "            labels.append(batch_labels)\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    embeddings = embeddings.squeeze()\n",
    "    labels = torch.cat(labels)\n",
    "    return embeddings, labels\n",
    "\n",
    "class One_shot_Dataset_hw(Dataset):\n",
    "    \n",
    "    def __init__(self, symbols, targets, transform=None):\n",
    "        self.symbols = symbols\n",
    "        self.targets = targets \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.symbols)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        symbol_image = cv2.imread(self.symbols[idx])\n",
    "        h, w = symbol_image.shape[:2]\n",
    "        symbol_image = Image.fromarray(symbol_image)\n",
    "        if self.transform:\n",
    "            symbol_image = self.transform(symbol_image)\n",
    "\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        return symbol_image, label, h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dir = '../Datasets/asupid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=1\n",
    "\n",
    "# Support set\n",
    "support_symbols, support_targets = get_m_symbols(classes_dir, m=m, to_run=True)\n",
    "support_set_dict = get_support_set_dict(support_symbols, support_targets)\n",
    "\n",
    "# test set\n",
    "test_symbols, test_targets = get_image_files(classes_dir), [int(o.parent.name) for o in get_image_files(classes_dir)]\n",
    "\n",
    "# dataloader\n",
    "\n",
    "to_tensor = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # helps in removing the affect of color\n",
    "    ])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = One_shot_Dataset_test(symbols = support_symbols, targets= support_targets, transform=to_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = One_shot_Dataset_test(test_symbols, test_targets, transform=to_tensor)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "for ims,targs in train_loader:\n",
    "    #print(ims)\n",
    "    print(ims.shape, targs.shape)\n",
    "    break\n",
    "\n",
    "show_image_batch((ims,targs), items=32, cols=8, figsize=(8,4)) \n",
    "\n",
    "\n",
    "for ims,targs in test_loader:\n",
    "    print(ims.shape, targs.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "show_image_batch((ims,targs), items=32, cols=8, figsize=(8,4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "model = model.to(device)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prototypes = []\n",
    "proto_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        prototypes.append(outputs.squeeze())\n",
    "        proto_labels.append(labels)\n",
    "prototypes = torch.cat(prototypes)\n",
    "proto_labels = torch.cat(proto_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = []\n",
    "query_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        query_embeddings.append(outputs.squeeze())\n",
    "        query_labels.append(labels)\n",
    "query_embeddings = torch.cat(query_embeddings)\n",
    "query_labels = torch.cat(query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prototypes = F.normalize(prototypes, p=2, dim=1)  # Shape [44, 2048]\n",
    "query_embeddings = F.normalize(query_embeddings, p=2, dim=1)  # Shape [2000, 2048]\n",
    "\n",
    "# Compute the cosine similarity\n",
    "cosine_similarity = torch.mm(query_embeddings, prototypes.T)  # Shape [2000, 44]\n",
    "\n",
    "# Find the index of the closest prototype for each query image\n",
    "closest_prototype_indices = torch.argmax(cosine_similarity, dim=1)  # Shape [2000]\n",
    "\n",
    "# If you need the actual maximum similarity values as well:\n",
    "max_similarities, closest_prototype_indices = torch.max(cosine_similarity, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a587ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "for idx in closest_prototype_indices:\n",
    "    pred_labels.append(proto_labels[idx].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff432af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(query_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b39644",
   "metadata": {},
   "source": [
    "## Without Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1247f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_time in range(5):\n",
    "    m=1\n",
    "\n",
    "    # Support set\n",
    "    support_symbols, support_targets = get_m_symbols(classes_dir, m=m, to_run=True)\n",
    "    support_set_dict = get_support_set_dict(support_symbols, support_targets)\n",
    "\n",
    "    # test set\n",
    "    test_symbols, test_targets = get_image_files(classes_dir), [int(o.parent.name) for o in get_image_files(classes_dir)]\n",
    "\n",
    "    # dataloader\n",
    "\n",
    "    to_tensor = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # helps in removing the affect of color\n",
    "        ])\n",
    "\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    train_dataset = One_shot_Dataset_test(symbols = support_symbols, targets= support_targets, transform=to_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = One_shot_Dataset_test(test_symbols, test_targets, transform=to_tensor)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # Load the pre-trained ResNet50 model\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model = model.to(device)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    print('ready')\n",
    "\n",
    "    prototypes = []\n",
    "    proto_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            prototypes.append(outputs.squeeze())\n",
    "            proto_labels.append(labels)\n",
    "    prototypes = torch.cat(prototypes)\n",
    "    proto_labels = torch.cat(proto_labels)\n",
    "    print(prototypes.shape, len(proto_labels))\n",
    "\n",
    "\n",
    "    query_embeddings = []\n",
    "    query_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            query_embeddings.append(outputs.squeeze())\n",
    "            query_labels.append(labels)\n",
    "    query_embeddings = torch.cat(query_embeddings)\n",
    "    query_labels = torch.cat(query_labels)\n",
    "\n",
    "    print(query_embeddings.shape, len(query_labels))\n",
    "\n",
    "    prototypes = F.normalize(prototypes, p=2, dim=1)  # Shape [44, 2048]\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)  # Shape [2000, 2048]\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarity = torch.mm(query_embeddings, prototypes.T)  # Shape [2000, 44]\n",
    "\n",
    "    # Find the index of the closest prototype for each query image\n",
    "    closest_prototype_indices = torch.argmax(cosine_similarity, dim=1)  # Shape [2000]\n",
    "\n",
    "\n",
    "    pred_labels = []\n",
    "    for idx in closest_prototype_indices:\n",
    "        pred_labels.append(proto_labels[idx].item())\n",
    "\n",
    "    print(n_time, ' F1 score: ', f1_score(query_labels, pred_labels, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070299a9",
   "metadata": {},
   "source": [
    "## With Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce934734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_time in range(5):\n",
    "    regular_augs = A.Compose([\n",
    "        A.Resize(height = 224, width = 224),\n",
    "        #A.D4(p=1), ### present in old\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.GridDistortion(distort_limit=(-0.1, 0.1), p=0.5),\n",
    "        A.ShiftScaleRotate(rotate_limit = (-10,10),interpolation = 0, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.AdvancedBlur(p=0.2),\n",
    "        A.Defocus(radius=(3, 5), alias_blur=(0.05, 0.15), p=0.15),\n",
    "        #A.Equalize(p=1), ########## Binarise #####\n",
    "        A.GaussNoise(p=0.15),\n",
    "        A.GaussianBlur(p=0.2),\n",
    "        A.GlassBlur(p=0.1), #### simulate scanned dwgs\n",
    "        A.Morphological(scale=(2, 3), operation='dilation', p=0.2),\n",
    "        A.Morphological(scale=(2, 3), operation='erosion', p=0.2)\n",
    "    ])\n",
    "\n",
    "\n",
    "    albs = {'REGULAR': regular_augs}\n",
    "    alb_key = 'REGULAR'\n",
    "    # 1.1 Get m=1 symbol per class\n",
    "    m=1\n",
    "    support_symbols, support_targets = get_m_symbols(classes_dir, m=m, to_run=True)\n",
    "    support_set_dict = get_support_set_dict(support_symbols, support_targets)\n",
    "\n",
    "    alb_transform = albs[alb_key]\n",
    "\n",
    "    num_per_class = 100\n",
    "    batch_size = 64\n",
    "\n",
    "    ## Run and save for later reloading\n",
    "    train_syms, train_targs = augment_data(support_set_dict, num_per_class, alb_transform)\n",
    "    print('Total Train images and labels generated are: ',len(train_syms), len(train_targs))\n",
    "\n",
    "\n",
    "    train_dataset = One_shot_Dataset_st_list(symbols = train_syms, targets= train_targs, transform=to_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = One_shot_Dataset_test(test_symbols, test_targets, transform=to_tensor)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # Load the pre-trained ResNet50 model\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model = model.to(device)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    print('ready')\n",
    "\n",
    "    prototypes = []\n",
    "    proto_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            prototypes.append(outputs.squeeze())\n",
    "            proto_labels.append(labels)\n",
    "    prototypes = torch.cat(prototypes)\n",
    "    proto_labels = torch.cat(proto_labels)\n",
    "    print(prototypes.shape, len(proto_labels))\n",
    "\n",
    "\n",
    "    query_embeddings = []\n",
    "    query_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            query_embeddings.append(outputs.squeeze())\n",
    "            query_labels.append(labels)\n",
    "    query_embeddings = torch.cat(query_embeddings)\n",
    "    query_labels = torch.cat(query_labels)\n",
    "\n",
    "    print(query_embeddings.shape, len(query_labels))\n",
    "\n",
    "    prototypes = F.normalize(prototypes, p=2, dim=1)  # Shape [44, 2048]\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)  # Shape [2000, 2048]\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarity = torch.mm(query_embeddings, prototypes.T)  # Shape [2000, 44]\n",
    "\n",
    "    # Find the index of the closest prototype for each query image\n",
    "    closest_prototype_indices = torch.argmax(cosine_similarity, dim=1)  # Shape [2000]\n",
    "\n",
    "\n",
    "    pred_labels = []\n",
    "    for idx in closest_prototype_indices:\n",
    "        pred_labels.append(proto_labels[idx].item())\n",
    "\n",
    "    print(n_time, ' F1 score: ', f1_score(query_labels, pred_labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c571718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
